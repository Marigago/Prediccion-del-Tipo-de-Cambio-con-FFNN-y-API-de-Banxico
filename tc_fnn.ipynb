{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Obtención de datos\n",
        "\n",
        "Conéctense a la API de Banxico para descargar la serie histórica del tipo de cambio FIX.\n",
        "\n",
        "Usen al menos 2 años de datos para entrenar.\n",
        "\n",
        "Conserven los últimos 15 días como conjunto de prueba (test set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "from scipy.stats import boxcox\n",
        "from scipy.special import inv_boxcox\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "21O9h8lglXpH",
        "outputId": "10877e7f-aeb4-4e6e-f570-f2e57156310f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TipoCambio</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fecha</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1991-11-12</th>\n",
              "      <td>3.0735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991-11-13</th>\n",
              "      <td>3.0712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991-11-14</th>\n",
              "      <td>3.0718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991-11-15</th>\n",
              "      <td>3.0684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991-11-18</th>\n",
              "      <td>3.0673</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            TipoCambio\n",
              "Fecha                 \n",
              "1991-11-12      3.0735\n",
              "1991-11-13      3.0712\n",
              "1991-11-14      3.0718\n",
              "1991-11-15      3.0684\n",
              "1991-11-18      3.0673"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Tu token de Banxico\n",
        "token = \"b2b8aef3c6559490348dbc3e9bfbe878054a181310b39ac2e33793ac6186b216\"\n",
        "\n",
        "# Serie del tipo de cambio FIX (peso/dólar)\n",
        "serie_id = \"SF43718\"\n",
        "\n",
        "# Endpoint\n",
        "url = f\"https://www.banxico.org.mx/SieAPIRest/service/v1/series/{serie_id}/datos\"\n",
        "\n",
        "# Headers con el token\n",
        "headers = {\"Bmx-Token\": token}\n",
        "\n",
        "# Llamada a la API\n",
        "response = requests.get(url, headers=headers)\n",
        "data = response.json()\n",
        "\n",
        "# Extraer datos\n",
        "datos = data[\"bmx\"][\"series\"][0][\"datos\"]   \n",
        "\n",
        "# Convertir a DataFrame\n",
        "df = pd.DataFrame(datos)\n",
        "df[\"Fecha\"] = pd.to_datetime(df[\"fecha\"], dayfirst=True)\n",
        "df[\"TipoCambio\"] = pd.to_numeric(df[\"dato\"], errors=\"coerce\")\n",
        "df.set_index('Fecha', inplace=True) \n",
        "df.drop(columns=['fecha', 'dato'], inplace= True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tI6Qx25umZ6o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "datetime64[ns]\n",
            "(8520, 1)\n",
            "            TipoCambio\n",
            "Fecha                 \n",
            "1991-11-12      3.0735\n",
            "1991-11-13      3.0712\n",
            "1991-11-14      3.0718\n",
            "            TipoCambio\n",
            "Fecha                 \n",
            "2025-10-01     18.3477\n",
            "2025-10-02     18.4843\n",
            "2025-10-03     18.3902\n"
          ]
        }
      ],
      "source": [
        "print(df.index.dtype)       \n",
        "print(df.shape)\n",
        "print(df.head(3))\n",
        "print(df.tail(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.io as pio\n",
        "pio.renderers.default = \"browser\"\n",
        "\n",
        "fig = px.line(\n",
        "    df,\n",
        "    x=df.index,\n",
        "    y=\"TipoCambio\",\n",
        "    title=\"Tipo de cambio FIX (Peso/Dólar) - Banxico\",\n",
        "    labels={\"TipoCambio\": \"Pesos por USD\"}\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Días entre 2021-01-01 y hoy: 1736\n"
          ]
        }
      ],
      "source": [
        "fecha_inicio = datetime(2021, 1, 1)\n",
        "hoy = datetime.today()\n",
        "\n",
        "dias = (hoy - fecha_inicio).days\n",
        "print(f\"Días entre 2021-01-01 y hoy: {dias}\")\n",
        "\n",
        "# Asegúrate de que el índice sea datetime\n",
        "df.index = pd.to_datetime(df.index)\n",
        "\n",
        "# Filtrar desde 2021-01-01 hasta hoy\n",
        "fecha_inicio = \"2021-01-01\"\n",
        "hoy = pd.Timestamp.today()\n",
        "\n",
        "df = df.loc[fecha_inicio:hoy]\n",
        "\n",
        "# Graficar con Plotly\n",
        "fig = px.line(\n",
        "    df,\n",
        "    x=df.index,\n",
        "    y=\"TipoCambio\",\n",
        "    title=\"Tipo de cambio FIX (Peso/Dólar) - Banxico\",\n",
        "    labels={\"TipoCambio\": \"Pesos por USD\"}\n",
        ")\n",
        "\n",
        "# Mostrar gráfico interactivo\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Normalizar datos\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df[[\"TipoCambio\"]])\n",
        "\n",
        "# Función para crear ventanas\n",
        "def create_dataset(series, window=5):\n",
        "    X, y = [], []\n",
        "    for i in range(len(series)-window):\n",
        "        X.append(series[i:(i+window), 0])\n",
        "        y.append(series[i+window, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "window_size = 15\n",
        "X, y = create_dataset(df_scaled, window_size)\n",
        "\n",
        "# Separar en train y test (últimos 15 días)\n",
        "\n",
        "n_test = 15\n",
        "X_train, X_test = X[:-n_test], X[-n_test:]\n",
        "y_train, y_test = y[:-n_test], y[-n_test:]\n",
        "\n",
        "fechas_test = df.index[window_size:][-n_test:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.64018385, 0.65805793, 0.61816955, ..., 0.59159554, 0.6207777 ,\n",
              "        0.66528051],\n",
              "       [0.65805793, 0.61816955, 0.65191143, ..., 0.6207777 , 0.66528051,\n",
              "        0.69747209],\n",
              "       [0.61816955, 0.65191143, 0.66294594, ..., 0.66528051, 0.69747209,\n",
              "        0.67109871],\n",
              "       ...,\n",
              "       [0.44913183, 0.4416539 , 0.44466331, ..., 0.44174509, 0.42742759,\n",
              "        0.42301379],\n",
              "       [0.4416539 , 0.44466331, 0.41207047, ..., 0.42742759, 0.42301379,\n",
              "        0.41942073],\n",
              "       [0.44466331, 0.41207047, 0.41969432, ..., 0.42301379, 0.41942073,\n",
              "        0.41152331]], shape=(1168, 15))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([[0.41207047, 0.41969432, 0.42835777, 0.42961625, 0.4225031 ,\n",
              "        0.42100751, 0.42137229, 0.43253447, 0.42890494, 0.44174509,\n",
              "        0.42742759, 0.42301379, 0.41942073, 0.41152331, 0.39997811],\n",
              "       [0.41969432, 0.42835777, 0.42961625, 0.4225031 , 0.42100751,\n",
              "        0.42137229, 0.43253447, 0.42890494, 0.44174509, 0.42742759,\n",
              "        0.42301379, 0.41942073, 0.41152331, 0.39997811, 0.39031152],\n",
              "       [0.42835777, 0.42961625, 0.4225031 , 0.42100751, 0.42137229,\n",
              "        0.43253447, 0.42890494, 0.44174509, 0.42742759, 0.42301379,\n",
              "        0.41942073, 0.41152331, 0.39997811, 0.39031152, 0.36984752],\n",
              "       [0.42961625, 0.4225031 , 0.42100751, 0.42137229, 0.43253447,\n",
              "        0.42890494, 0.44174509, 0.42742759, 0.42301379, 0.41942073,\n",
              "        0.41152331, 0.39997811, 0.39031152, 0.36984752, 0.36295324],\n",
              "       [0.4225031 , 0.42100751, 0.42137229, 0.43253447, 0.42890494,\n",
              "        0.44174509, 0.42742759, 0.42301379, 0.41942073, 0.41152331,\n",
              "        0.39997811, 0.39031152, 0.36984752, 0.36295324, 0.36939155],\n",
              "       [0.42100751, 0.42137229, 0.43253447, 0.42890494, 0.44174509,\n",
              "        0.42742759, 0.42301379, 0.41942073, 0.41152331, 0.39997811,\n",
              "        0.39031152, 0.36984752, 0.36295324, 0.36939155, 0.37453491],\n",
              "       [0.42137229, 0.43253447, 0.42890494, 0.44174509, 0.42742759,\n",
              "        0.42301379, 0.41942073, 0.41152331, 0.39997811, 0.39031152,\n",
              "        0.36984752, 0.36295324, 0.36939155, 0.37453491, 0.37800029],\n",
              "       [0.43253447, 0.42890494, 0.44174509, 0.42742759, 0.42301379,\n",
              "        0.41942073, 0.41152331, 0.39997811, 0.39031152, 0.36984752,\n",
              "        0.36295324, 0.36939155, 0.37453491, 0.37800029, 0.36249726],\n",
              "       [0.42890494, 0.44174509, 0.42742759, 0.42301379, 0.41942073,\n",
              "        0.41152331, 0.39997811, 0.39031152, 0.36984752, 0.36295324,\n",
              "        0.36939155, 0.37453491, 0.37800029, 0.36249726, 0.38219523],\n",
              "       [0.44174509, 0.42742759, 0.42301379, 0.41942073, 0.41152331,\n",
              "        0.39997811, 0.39031152, 0.36984752, 0.36295324, 0.36939155,\n",
              "        0.37453491, 0.37800029, 0.36249726, 0.38219523, 0.38483986],\n",
              "       [0.42742759, 0.42301379, 0.41942073, 0.41152331, 0.39997811,\n",
              "        0.39031152, 0.36984752, 0.36295324, 0.36939155, 0.37453491,\n",
              "        0.37800029, 0.36249726, 0.38219523, 0.38483986, 0.37331291],\n",
              "       [0.42301379, 0.41942073, 0.41152331, 0.39997811, 0.39031152,\n",
              "        0.36984752, 0.36295324, 0.36939155, 0.37453491, 0.37800029,\n",
              "        0.36249726, 0.38219523, 0.38483986, 0.37331291, 0.36751295],\n",
              "       [0.41942073, 0.41152331, 0.39997811, 0.39031152, 0.36984752,\n",
              "        0.36295324, 0.36939155, 0.37453491, 0.37800029, 0.36249726,\n",
              "        0.38219523, 0.38483986, 0.37331291, 0.36751295, 0.36450354],\n",
              "       [0.41152331, 0.39997811, 0.39031152, 0.36984752, 0.36295324,\n",
              "        0.36939155, 0.37453491, 0.37800029, 0.36249726, 0.38219523,\n",
              "        0.38483986, 0.37331291, 0.36751295, 0.36450354, 0.36696578],\n",
              "       [0.39997811, 0.39031152, 0.36984752, 0.36295324, 0.36939155,\n",
              "        0.37453491, 0.37800029, 0.36249726, 0.38219523, 0.38483986,\n",
              "        0.37331291, 0.36751295, 0.36450354, 0.36696578, 0.39188006]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([0.69747209, 0.67109871, 0.70786824, ..., 0.41942073, 0.41152331,\n",
              "       0.39997811], shape=(1168,))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([0.39031152, 0.36984752, 0.36295324, 0.36939155, 0.37453491,\n",
              "       0.37800029, 0.36249726, 0.38219523, 0.38483986, 0.37331291,\n",
              "       0.36751295, 0.36450354, 0.36696578, 0.39188006, 0.3747173 ])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(X_train)\n",
        "display(X_test)\n",
        "display(y_train)\n",
        "display(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preparación de datos\n",
        "\n",
        "Normalicen los datos (ejemplo: MinMaxScaler).\n",
        "\n",
        "Construyan ventanas deslizantes de tamaño definido por ustedes (ejemplo: 10, 15, 30 días).\n",
        "\n",
        "Justifiquen el tamaño de la ventana: ¿por qué ese número de rezagos es razonable?\n",
        "R = Dado que vamos a predecir 15 días, probamos un aprendizaje de 15 días por 15 días. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "# ==============================\n",
        "# PARÁMETROS (ajustables rápido)\n",
        "# ==============================\n",
        "WINDOW_SIZE = 10        # Número de columnas de ventana deslizante (len)\n",
        "HIDDEN_LAYERS = [64, 32]  # Neuronas por capa oculta (puedes agregar/quitar capas)\n",
        "ACTIVATION = \"relu\"     # Función de activación (relu, tanh, sigmoid, etc.)\n",
        "OUTPUT_ACTIVATION = \"linear\"  # Salida para regresión\n",
        "OPTIMIZER = Adam(learning_rate=0.001)  # Optimizador\n",
        "EPOCHS = 50             # Número de épocas\n",
        "BATCH_SIZE = 16         # Tamaño del batch\n",
        "LOSS = \"mse\"            # Error cuadrático medio para regresión\n",
        "METRICS = [\"mae\"]       # Métrica adicional (error absoluto medio)\n",
        "\n",
        "# ==============================\n",
        "# FUNCIONES PRINCIPALES\n",
        "# ==============================\n",
        "\n",
        "def preparar_datos(df, window_size):\n",
        "    \"\"\"\n",
        "    Separa X e y del DataFrame.\n",
        "    Asume que las últimas columnas después de 'fecha' y 'dolar' son la ventana deslizante.\n",
        "    \"\"\"\n",
        "    X = df.iloc[:, -window_size:].values  # Últimas columnas = ventana\n",
        "    y = df[\"dolar\"].values                # Columna objetivo\n",
        "    return X, y\n",
        "\n",
        "def construir_modelo(input_dim, hidden_layers, activation, output_activation, optimizer, loss, metrics):\n",
        "    \"\"\"\n",
        "    Construcción de la red neuronal feed-forward (FFNN).\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Capa de entrada + primera oculta\n",
        "    model.add(Dense(hidden_layers[0], activation=activation, input_dim=input_dim))\n",
        "    \n",
        "    # Capas ocultas adicionales\n",
        "    for units in hidden_layers[1:]:\n",
        "        model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # Capa de salida (regresión → 1 neurona)\n",
        "    model.add(Dense(1, activation=output_activation))\n",
        "    \n",
        "    # Compilar\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def entrenar_modelo(model, X, y, epochs, batch_size):\n",
        "    \"\"\"\n",
        "    Entrena el modelo con los datos dados.\n",
        "    \"\"\"\n",
        "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n",
        "    return history\n",
        "\n",
        "# ==============================\n",
        "# MAIN\n",
        "# ==============================\n",
        "\n",
        "def main(df):\n",
        "    # Preparar datos\n",
        "    X, y = preparar_datos(df, WINDOW_SIZE)\n",
        "    \n",
        "    # Construir modelo\n",
        "    model = construir_modelo(\n",
        "        input_dim=X.shape[1],\n",
        "        hidden_layers=HIDDEN_LAYERS,\n",
        "        activation=ACTIVATION,\n",
        "        output_activation=OUTPUT_ACTIVATION,\n",
        "        optimizer=OPTIMIZER,\n",
        "        loss=LOSS,\n",
        "        metrics=METRICS\n",
        "    )\n",
        "    \n",
        "    # Entrenar modelo\n",
        "    history = entrenar_modelo(model, X, y, EPOCHS, BATCH_SIZE)\n",
        "    \n",
        "    return model, history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Construcción del modelo FFNN\n",
        "\n",
        "### ELEGIMOS EL MODELO 1 (Remi/Ivo) PORQUE SALE MENOS MAPE\n",
        "Definan una red neuronal secuencial en Keras con:\n",
        "\n",
        "Una o más capas densas ocultas con funciones de activación no lineales (ej. ReLU, tanh).\n",
        "\n",
        "Una capa de salida con 1 neurona (valor del tipo de cambio).\n",
        "\n",
        "Seleccionen y documenten sus hiperparámetros: número de capas, neuronas, epochs, batch size, optimizador.\n",
        "\n",
        "Expliquen brevemente por qué tomaron esas decisiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Componente                | Valor en tu código                                         | ¿Por qué así?                                                                                                                                                        | Qué optimiza / trade-off                                                                                     |\n",
        "| ------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
        "| **Arquitectura**          | `Dense(64, relu)` → `Dense(32, relu)` → `Dense(1, linear)` | Estructura MLP “clásica” de 2 capas ocultas que **captura no linealidades** sin irse a un modelo demasiado grande (riesgo de sobreajuste con series de 2–5k puntos). | Balancea **capacidad** vs **generalización**. Dos capas suelen ser suficiente para patrones de ventana fija. |\n",
        "| **Neuronas**              | 64 → 32                                                    | Regla práctica: empezar con 2–4× el tamaño de entrada (ventana 15 ⇒ 30–60). 64/32 ofrece capacidad extra para no linealidades suaves.                                | +Capacidad para relaciones complejas. −Mayor riesgo de overfit si no regularizas.                            |\n",
        "| **Función de activación** | `relu` en ocultas, `linear` en salida                      | `relu` es estable y barata; `linear` en salida porque el objetivo es **regresión** (valor continuo).                                                                 | `relu` acelera convergencia. `linear` evita saturar salidas.                                                 |\n",
        "| **Optimizador**           | `adam` (default LR=1e-3)                                   | En series univariadas con MLP, **Adam** converge rápido y tolera escalas distintas tras normalización.                                                               | +Velocidad de entrenamiento. −Puede “estancarse” si LR no se ajusta.                                         |\n",
        "| **Pérdida**               | `mse`                                                      | Penaliza más los errores grandes; estable para regresión.                                                                                                            | +Estabilidad. −Más sensible a outliers que MAE/Huber.                                                        |\n",
        "| **Epochs**                | `EPOCHS=50`                                                | Con ventanas deslizantes y validación 10%, 50 épocas suelen ser suficientes con Adam para converger sin sobreentrenar.                                               | +Tiempo corto. −Si hay estacionalidades largas puede faltar capacidad; suple con *early stopping*.           |\n",
        "| **Batch size**            | `BATCH_SIZE=16`                                            | Tamaño pequeño mejora ruido estocástico (regularización implícita) y cabe en RAM/GPU.                                                                                | +Generalización. −Entrenamiento algo más lento.                                                              |\n",
        "| **Semilla (seed)**        | `SEED=40`                                                  | Reproducibilidad total (NumPy + TF).                                                                                                                                 | Comparabilidad de experimentos.                                                                              |\n",
        "| **Ventana temporal**      | `window_size=15`                                           | Captura ~3 semanas hábiles (mercado FX). Buen punto medio si hay **inercia** de corto plazo.                                                                         | +Simplicidad. −Puede omitir ciclos >15 días (mensualidad, estacionalidad).                                   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODELO 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0208 - val_loss: 0.0012\n",
            "Epoch 2/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579us/step - loss: 0.0012 - val_loss: 8.2067e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 9.9101e-04 - val_loss: 6.5340e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 9.0821e-04 - val_loss: 5.9160e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 8.5154e-04 - val_loss: 5.5331e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 8.2613e-04 - val_loss: 5.0812e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - loss: 8.0756e-04 - val_loss: 4.5840e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 7.9182e-04 - val_loss: 4.2247e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 7.6348e-04 - val_loss: 4.1238e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - loss: 7.4031e-04 - val_loss: 4.0803e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - loss: 7.2117e-04 - val_loss: 4.0447e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 7.0640e-04 - val_loss: 4.0251e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - loss: 7.0012e-04 - val_loss: 3.9626e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 6.9549e-04 - val_loss: 3.9751e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 6.9174e-04 - val_loss: 3.9596e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 6.8845e-04 - val_loss: 3.9386e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - loss: 6.8800e-04 - val_loss: 3.9636e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 6.8542e-04 - val_loss: 3.9559e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 6.8412e-04 - val_loss: 3.9373e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - loss: 6.8471e-04 - val_loss: 3.9819e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step - loss: 6.8414e-04 - val_loss: 3.9528e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - loss: 6.8524e-04 - val_loss: 3.9518e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 540us/step - loss: 6.8583e-04 - val_loss: 3.9262e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 6.8867e-04 - val_loss: 3.9271e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563us/step - loss: 6.8920e-04 - val_loss: 3.9520e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 6.9076e-04 - val_loss: 3.9362e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - loss: 6.9092e-04 - val_loss: 3.9735e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 6.9094e-04 - val_loss: 3.9809e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 6.9136e-04 - val_loss: 4.0379e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 6.9047e-04 - val_loss: 4.0349e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - loss: 6.9065e-04 - val_loss: 3.9809e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - loss: 6.9047e-04 - val_loss: 4.0506e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - loss: 6.8926e-04 - val_loss: 4.1022e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - loss: 6.8700e-04 - val_loss: 4.0839e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - loss: 6.8102e-04 - val_loss: 4.1153e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 6.7473e-04 - val_loss: 4.0323e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - loss: 6.7866e-04 - val_loss: 4.0327e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - loss: 6.8038e-04 - val_loss: 4.1664e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 6.7721e-04 - val_loss: 4.1542e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - loss: 6.7708e-04 - val_loss: 4.1940e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 6.7760e-04 - val_loss: 4.1466e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - loss: 6.7632e-04 - val_loss: 4.2223e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - loss: 6.7960e-04 - val_loss: 4.2413e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 6.7618e-04 - val_loss: 4.2146e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 6.7798e-04 - val_loss: 4.2582e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step - loss: 6.7610e-04 - val_loss: 4.1841e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - loss: 6.7969e-04 - val_loss: 4.1185e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - loss: 6.7653e-04 - val_loss: 4.0941e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 6.7591e-04 - val_loss: 4.1080e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - loss: 6.7248e-04 - val_loss: 4.1633e-04\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "MAPE en test: 0.33%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "# =====================\n",
        "# Parámetros globales\n",
        "# =====================\n",
        "SEED = 40\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "window_size = 15  # mismo usado al crear X, y\n",
        "\n",
        "# Reproducibilidad\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# =====================\n",
        "# 1. Construcción modelo\n",
        "# =====================\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation=\"relu\", input_shape=(window_size,)))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"linear\"))  # salida escalar\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "# =====================\n",
        "# 2. Entrenamiento\n",
        "# =====================\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# =====================\n",
        "# 3. Predicciones\n",
        "# =====================\n",
        "# Predicciones en todo el set\n",
        "y_pred_all = model.predict(np.vstack([X_train, X_test]))\n",
        "y_pred_all_rescaled = scaler.inverse_transform(y_pred_all.reshape(-1, 1))\n",
        "y_all_rescaled = scaler.inverse_transform(np.concatenate([y_train.reshape(-1,1), y_test.reshape(-1,1)], axis=0))\n",
        "\n",
        "\n",
        "# Fechas correspondientes a y (después de la ventana deslizante)\n",
        "fechas_all = df.index[window_size:]\n",
        "\n",
        "df_pred_all = pd.DataFrame({\n",
        "    \"Real\": y_all_rescaled.flatten(),\n",
        "    \"Predicho\": y_pred_all_rescaled.flatten()\n",
        "}, index=fechas_all)\n",
        "\n",
        "# Predicciones solo test (últimos 15 días)\n",
        "y_pred_test = model.predict(X_test)\n",
        "y_pred_test_rescaled = scaler.inverse_transform(y_pred_test.reshape(-1, 1))\n",
        "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "fechas_test = df.index[-len(y_test):]\n",
        "df_pred = pd.DataFrame({\n",
        "    \"Real\": y_test_rescaled.flatten(),\n",
        "    \"Predicho\": y_pred_test_rescaled.flatten()\n",
        "}, index=fechas_test)\n",
        "\n",
        "# =====================\n",
        "# 4. Evaluación\n",
        "# =====================\n",
        "mape = mean_absolute_percentage_error(y_test_rescaled, y_pred_test_rescaled) * 100\n",
        "print(f\"MAPE en test: {mape:.2f}%\")\n",
        "\n",
        "# =====================\n",
        "# 5. Gráfica con Plotly\n",
        "# =====================\n",
        "fecha_inicio = \"2015-01-01\"\n",
        "df_filtrado = df.loc[fecha_inicio:]\n",
        "df_pred_all_filtrado = df_pred_all.loc[fecha_inicio:]\n",
        "df_pred_filtrado = df_pred.loc[fecha_inicio:]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Serie completa real\n",
        "fig.add_trace(go.Scatter(x=df_filtrado.index, y=df_filtrado[\"TipoCambio\"],\n",
        "                         mode=\"lines\",\n",
        "                         name=\"Serie completa (Real)\",\n",
        "                         line=dict(color=\"lightgray\")))\n",
        "\n",
        "# Predicciones de todo el modelo (train + test)\n",
        "fig.add_trace(go.Scatter(x=df_pred_all_filtrado.index, y=df_pred_all_filtrado[\"Predicho\"],\n",
        "                         mode=\"lines\",\n",
        "                         name=\"Predicho (train + test)\",\n",
        "                         line=dict(color=\"orange\")))\n",
        "\n",
        "# Últimos 15 días - reales\n",
        "fig.add_trace(go.Scatter(x=df_pred_filtrado.index, y=df_pred_filtrado[\"Real\"],\n",
        "                         mode=\"lines+markers\",\n",
        "                         name=\"Real (últimos 15 días)\",\n",
        "                         line=dict(color=\"blue\")))\n",
        "\n",
        "# Últimos 15 días - predichos\n",
        "fig.add_trace(go.Scatter(x=df_pred_filtrado.index, y=df_pred_filtrado[\"Predicho\"],\n",
        "                         mode=\"lines+markers\",\n",
        "                         name=\"Predicho (últimos 15 días)\",\n",
        "                         line=dict(color=\"red\", dash=\"dot\")))\n",
        "\n",
        "fig.update_layout(title=\"Predicción del tipo de cambio USD/MXN con FFNN\",\n",
        "                  xaxis_title=\"Fecha\",\n",
        "                  yaxis_title=\"USD/MXN\",\n",
        "                  legend=dict(x=0.01, y=0.99, bordercolor=\"black\", borderwidth=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelo 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1001 - val_loss: 0.0099\n",
            "Epoch 2/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0063 - val_loss: 0.0018\n",
            "Epoch 3/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0034 - val_loss: 0.0015\n",
            "Epoch 4/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 0.0019 - val_loss: 9.5331e-04\n",
            "Epoch 5/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.0014 - val_loss: 6.7286e-04\n",
            "Epoch 6/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - loss: 9.3669e-04 - val_loss: 4.9265e-04\n",
            "Epoch 7/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - loss: 8.5376e-04 - val_loss: 5.3893e-04\n",
            "Epoch 8/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 8.3626e-04 - val_loss: 4.5842e-04\n",
            "Epoch 9/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.9699e-04 - val_loss: 4.6592e-04\n",
            "Epoch 10/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.9249e-04 - val_loss: 4.5493e-04\n",
            "Epoch 11/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.8030e-04 - val_loss: 4.5762e-04\n",
            "Epoch 12/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - loss: 7.8202e-04 - val_loss: 4.5579e-04\n",
            "Epoch 13/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.7814e-04 - val_loss: 4.5760e-04\n",
            "Epoch 14/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 7.7358e-04 - val_loss: 4.4984e-04\n",
            "Epoch 15/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 7.6980e-04 - val_loss: 4.5695e-04\n",
            "Epoch 16/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 7.7098e-04 - val_loss: 4.4502e-04\n",
            "Epoch 17/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 7.6683e-04 - val_loss: 4.7484e-04\n",
            "Epoch 18/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 7.6942e-04 - val_loss: 4.7746e-04\n",
            "Epoch 19/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 7.6095e-04 - val_loss: 4.4765e-04\n",
            "Epoch 20/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 7.4313e-04 - val_loss: 4.6590e-04\n",
            "Epoch 21/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 7.5348e-04 - val_loss: 4.5601e-04\n",
            "Epoch 22/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 7.4195e-04 - val_loss: 4.6552e-04\n",
            "Epoch 23/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.4510e-04 - val_loss: 4.4976e-04\n",
            "Epoch 24/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 7.3869e-04 - val_loss: 4.6331e-04\n",
            "Epoch 25/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 7.4121e-04 - val_loss: 4.6678e-04\n",
            "Epoch 26/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 7.3992e-04 - val_loss: 4.5615e-04\n",
            "Epoch 27/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 7.3783e-04 - val_loss: 4.5794e-04\n",
            "Epoch 28/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 7.3759e-04 - val_loss: 4.9019e-04\n",
            "Epoch 29/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 7.4120e-04 - val_loss: 4.4009e-04\n",
            "Epoch 30/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 7.1623e-04 - val_loss: 4.3405e-04\n",
            "Epoch 31/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 7.2590e-04 - val_loss: 4.9762e-04\n",
            "Epoch 32/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4016e-04 - val_loss: 4.5550e-04\n",
            "Epoch 33/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 7.1133e-04 - val_loss: 4.2092e-04\n",
            "Epoch 34/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 7.1921e-04 - val_loss: 5.0535e-04\n",
            "Epoch 35/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 7.3611e-04 - val_loss: 4.3712e-04\n",
            "Epoch 36/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 7.0303e-04 - val_loss: 4.2185e-04\n",
            "Epoch 37/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 7.1560e-04 - val_loss: 5.0541e-04\n",
            "Epoch 38/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 7.3253e-04 - val_loss: 4.4791e-04\n",
            "Epoch 39/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 7.0357e-04 - val_loss: 4.2263e-04\n",
            "Epoch 40/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 7.1321e-04 - val_loss: 5.0998e-04\n",
            "Epoch 41/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 7.3277e-04 - val_loss: 4.5998e-04\n",
            "Epoch 42/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 7.0381e-04 - val_loss: 4.1459e-04\n",
            "Epoch 43/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 7.0582e-04 - val_loss: 5.1567e-04\n",
            "Epoch 44/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 7.2883e-04 - val_loss: 4.5073e-04\n",
            "Epoch 45/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - loss: 6.9650e-04 - val_loss: 4.1803e-04\n",
            "Epoch 46/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 7.0443e-04 - val_loss: 4.9926e-04\n",
            "Epoch 47/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 7.1774e-04 - val_loss: 4.5200e-04\n",
            "Epoch 48/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.9371e-04 - val_loss: 4.1880e-04\n",
            "Epoch 49/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 7.0468e-04 - val_loss: 5.1805e-04\n",
            "Epoch 50/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 7.1849e-04 - val_loss: 4.3918e-04\n",
            "Epoch 51/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 6.8653e-04 - val_loss: 4.2195e-04\n",
            "Epoch 52/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 7.0106e-04 - val_loss: 5.1337e-04\n",
            "Epoch 53/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - loss: 7.1836e-04 - val_loss: 4.4900e-04\n",
            "Epoch 54/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 6.8514e-04 - val_loss: 4.1742e-04\n",
            "Epoch 55/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 6.9238e-04 - val_loss: 4.9611e-04\n",
            "Epoch 56/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 7.0327e-04 - val_loss: 4.7179e-04\n",
            "Epoch 57/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 6.9983e-04 - val_loss: 4.4470e-04\n",
            "Epoch 58/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 6.7767e-04 - val_loss: 4.2324e-04\n",
            "Epoch 59/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 6.7960e-04 - val_loss: 4.5870e-04\n",
            "Epoch 60/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 6.8903e-04 - val_loss: 4.7649e-04\n",
            "Epoch 61/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 6.9275e-04 - val_loss: 4.6201e-04\n",
            "Epoch 62/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 6.8509e-04 - val_loss: 4.5633e-04\n",
            "Epoch 63/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 6.7924e-04 - val_loss: 4.3282e-04\n",
            "Epoch 64/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 6.8641e-04 - val_loss: 4.9592e-04\n",
            "Epoch 65/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.9949e-04 - val_loss: 5.0241e-04\n",
            "Epoch 66/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 6.8577e-04 - val_loss: 4.3260e-04\n",
            "Epoch 67/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 6.7852e-04 - val_loss: 4.4007e-04\n",
            "Epoch 68/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 6.7073e-04 - val_loss: 4.3775e-04\n",
            "Epoch 69/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 6.7586e-04 - val_loss: 4.4337e-04\n",
            "Epoch 70/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 6.9237e-04 - val_loss: 5.6715e-04\n",
            "Epoch 71/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 7.0158e-04 - val_loss: 4.1383e-04\n",
            "Epoch 72/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 6.7643e-04 - val_loss: 5.0326e-04\n",
            "Epoch 73/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 6.8412e-04 - val_loss: 4.7872e-04\n",
            "Epoch 74/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 6.9410e-04 - val_loss: 5.0966e-04\n",
            "Epoch 75/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 6.8475e-04 - val_loss: 4.6512e-04\n",
            "Epoch 76/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 6.7728e-04 - val_loss: 4.7124e-04\n",
            "Epoch 77/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 6.6567e-04 - val_loss: 4.3830e-04\n",
            "Epoch 78/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 6.6455e-04 - val_loss: 4.4222e-04\n",
            "Epoch 79/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 6.7293e-04 - val_loss: 5.2485e-04\n",
            "Epoch 80/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.8680e-04 - val_loss: 4.3937e-04\n",
            "Epoch 81/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.6317e-04 - val_loss: 4.4383e-04\n",
            "Epoch 82/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 6.7741e-04 - val_loss: 5.5004e-04\n",
            "Epoch 83/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 6.7949e-04 - val_loss: 4.1612e-04\n",
            "Epoch 84/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 6.7563e-04 - val_loss: 5.2203e-04\n",
            "Epoch 85/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 6.7447e-04 - val_loss: 4.3116e-04\n",
            "Epoch 86/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 6.8037e-04 - val_loss: 5.5735e-04\n",
            "Epoch 87/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 6.7356e-04 - val_loss: 4.1964e-04\n",
            "Epoch 88/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 6.7492e-04 - val_loss: 5.2018e-04\n",
            "Epoch 89/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 6.7458e-04 - val_loss: 5.0515e-04\n",
            "Epoch 90/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 6.8083e-04 - val_loss: 4.7175e-04\n",
            "Epoch 91/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 6.6035e-04 - val_loss: 4.3674e-04\n",
            "Epoch 92/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - loss: 6.6848e-04 - val_loss: 5.2747e-04\n",
            "Epoch 93/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 6.6703e-04 - val_loss: 4.4132e-04\n",
            "Epoch 94/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 6.7989e-04 - val_loss: 5.6826e-04\n",
            "Epoch 95/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - loss: 6.7701e-04 - val_loss: 4.2229e-04\n",
            "Epoch 96/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 6.6289e-04 - val_loss: 5.2724e-04\n",
            "Epoch 97/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 6.6565e-04 - val_loss: 4.3287e-04\n",
            "Epoch 98/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 6.7262e-04 - val_loss: 5.6888e-04\n",
            "Epoch 99/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 6.7114e-04 - val_loss: 4.2375e-04\n",
            "Epoch 100/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 6.5034e-04 - val_loss: 4.7390e-04\n",
            "Epoch 101/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 6.5208e-04 - val_loss: 4.4284e-04\n",
            "Epoch 102/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 6.7225e-04 - val_loss: 5.4426e-04\n",
            "Epoch 103/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 6.6885e-04 - val_loss: 4.3341e-04\n",
            "Epoch 104/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 6.7341e-04 - val_loss: 5.3440e-04\n",
            "Epoch 105/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 6.5847e-04 - val_loss: 4.2647e-04\n",
            "Epoch 106/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 6.6522e-04 - val_loss: 5.1895e-04\n",
            "Epoch 107/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 6.7491e-04 - val_loss: 4.5509e-04\n",
            "Epoch 108/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 6.4390e-04 - val_loss: 4.3197e-04\n",
            "Epoch 109/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - loss: 6.6138e-04 - val_loss: 5.5215e-04\n",
            "Epoch 110/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 6.6649e-04 - val_loss: 4.3195e-04\n",
            "Epoch 111/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 6.7117e-04 - val_loss: 5.0078e-04\n",
            "Epoch 112/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 6.5615e-04 - val_loss: 4.1652e-04\n",
            "Epoch 113/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 6.4809e-04 - val_loss: 4.8819e-04\n",
            "Epoch 114/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 6.4899e-04 - val_loss: 4.4633e-04\n",
            "Epoch 115/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 6.6985e-04 - val_loss: 5.3355e-04\n",
            "Epoch 116/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 6.7380e-04 - val_loss: 4.2479e-04\n",
            "Epoch 117/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 6.4566e-04 - val_loss: 5.0765e-04\n",
            "Epoch 118/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 6.5420e-04 - val_loss: 4.1904e-04\n",
            "Epoch 119/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 6.5553e-04 - val_loss: 5.8078e-04\n",
            "Epoch 120/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 6.9544e-04 - val_loss: 4.8442e-04\n",
            "Epoch 121/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 6.5339e-04 - val_loss: 4.4468e-04\n",
            "Epoch 122/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 6.5326e-04 - val_loss: 5.6229e-04\n",
            "Epoch 123/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 6.8004e-04 - val_loss: 4.8125e-04\n",
            "Epoch 124/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 6.4638e-04 - val_loss: 4.4885e-04\n",
            "Epoch 125/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 6.4524e-04 - val_loss: 4.8648e-04\n",
            "Epoch 126/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 6.4041e-04 - val_loss: 4.3100e-04\n",
            "Epoch 127/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 6.3854e-04 - val_loss: 4.8330e-04\n",
            "Epoch 128/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 6.4404e-04 - val_loss: 4.5409e-04\n",
            "Epoch 129/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 6.4749e-04 - val_loss: 4.7693e-04\n",
            "Epoch 130/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 6.5099e-04 - val_loss: 4.7436e-04\n",
            "Epoch 131/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 6.5391e-04 - val_loss: 4.9155e-04\n",
            "Epoch 132/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 6.5512e-04 - val_loss: 4.5962e-04\n",
            "Epoch 133/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 6.5123e-04 - val_loss: 4.8921e-04\n",
            "Epoch 134/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 6.4540e-04 - val_loss: 4.4140e-04\n",
            "Epoch 135/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 6.3357e-04 - val_loss: 4.5796e-04\n",
            "Epoch 136/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 6.3731e-04 - val_loss: 4.6269e-04\n",
            "Epoch 137/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 6.3717e-04 - val_loss: 4.5887e-04\n",
            "Epoch 138/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 6.4069e-04 - val_loss: 4.6509e-04\n",
            "Epoch 139/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 6.3752e-04 - val_loss: 4.5984e-04\n",
            "Epoch 140/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.4446e-04 - val_loss: 4.7325e-04\n",
            "Epoch 141/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.3875e-04 - val_loss: 4.5822e-04\n",
            "Epoch 142/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 6.3804e-04 - val_loss: 4.6775e-04\n",
            "Epoch 143/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 6.3471e-04 - val_loss: 4.6233e-04\n",
            "Epoch 144/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 6.4251e-04 - val_loss: 4.8303e-04\n",
            "Epoch 145/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 6.3923e-04 - val_loss: 4.6104e-04\n",
            "Epoch 146/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 6.3921e-04 - val_loss: 4.7701e-04\n",
            "Epoch 147/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 6.4220e-04 - val_loss: 4.6077e-04\n",
            "Epoch 148/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3736e-04 - val_loss: 4.6731e-04\n",
            "Epoch 149/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 6.3186e-04 - val_loss: 4.6759e-04\n",
            "Epoch 150/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 6.3671e-04 - val_loss: 4.7621e-04\n",
            "Epoch 151/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 6.3493e-04 - val_loss: 4.7026e-04\n",
            "Epoch 152/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 6.3771e-04 - val_loss: 4.7065e-04\n",
            "Epoch 153/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 6.3410e-04 - val_loss: 4.6450e-04\n",
            "Epoch 154/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 6.3453e-04 - val_loss: 4.7704e-04\n",
            "Epoch 155/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 6.3620e-04 - val_loss: 4.6724e-04\n",
            "Epoch 156/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 6.3351e-04 - val_loss: 4.6451e-04\n",
            "Epoch 157/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 6.3468e-04 - val_loss: 4.7259e-04\n",
            "Epoch 158/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.3487e-04 - val_loss: 4.6788e-04\n",
            "Epoch 159/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 6.3452e-04 - val_loss: 4.7430e-04\n",
            "Epoch 160/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 6.3823e-04 - val_loss: 4.7468e-04\n",
            "Epoch 161/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.3511e-04 - val_loss: 4.6359e-04\n",
            "Epoch 162/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 6.3067e-04 - val_loss: 4.7208e-04\n",
            "Epoch 163/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 6.2934e-04 - val_loss: 4.5795e-04\n",
            "Epoch 164/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 6.3111e-04 - val_loss: 4.7738e-04\n",
            "Epoch 165/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 6.3326e-04 - val_loss: 4.6640e-04\n",
            "Epoch 166/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 6.2862e-04 - val_loss: 4.6059e-04\n",
            "Epoch 167/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 6.3094e-04 - val_loss: 4.9740e-04\n",
            "Epoch 168/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 6.3492e-04 - val_loss: 4.5858e-04\n",
            "Epoch 169/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 6.2819e-04 - val_loss: 4.7630e-04\n",
            "Epoch 170/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - loss: 6.3215e-04 - val_loss: 4.7211e-04\n",
            "Epoch 171/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 6.2353e-04 - val_loss: 4.5705e-04\n",
            "Epoch 172/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - loss: 6.2532e-04 - val_loss: 4.7344e-04\n",
            "Epoch 173/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 6.2513e-04 - val_loss: 4.6797e-04\n",
            "Epoch 174/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 6.2874e-04 - val_loss: 4.6469e-04\n",
            "Epoch 175/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 6.2900e-04 - val_loss: 4.5702e-04\n",
            "Epoch 176/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 6.2369e-04 - val_loss: 4.8660e-04\n",
            "Epoch 177/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 6.2425e-04 - val_loss: 4.5635e-04\n",
            "Epoch 178/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 6.2432e-04 - val_loss: 4.9957e-04\n",
            "Epoch 179/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 6.3064e-04 - val_loss: 4.6947e-04\n",
            "Epoch 180/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 6.2397e-04 - val_loss: 4.6561e-04\n",
            "Epoch 181/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 6.2796e-04 - val_loss: 4.7911e-04\n",
            "Epoch 182/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 6.2925e-04 - val_loss: 4.6310e-04\n",
            "Epoch 183/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 6.2658e-04 - val_loss: 4.6355e-04\n",
            "Epoch 184/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1631e-04 - val_loss: 4.6431e-04\n",
            "Epoch 185/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 6.1936e-04 - val_loss: 4.9693e-04\n",
            "Epoch 186/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 6.2269e-04 - val_loss: 4.4504e-04\n",
            "Epoch 187/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - loss: 6.1883e-04 - val_loss: 4.6471e-04\n",
            "Epoch 188/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 6.2098e-04 - val_loss: 4.6290e-04\n",
            "Epoch 189/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 6.1689e-04 - val_loss: 4.5705e-04\n",
            "Epoch 190/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 6.1645e-04 - val_loss: 4.7926e-04\n",
            "Epoch 191/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 6.2158e-04 - val_loss: 4.6180e-04\n",
            "Epoch 192/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 6.1644e-04 - val_loss: 4.7919e-04\n",
            "Epoch 193/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 6.1849e-04 - val_loss: 4.5739e-04\n",
            "Epoch 194/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1794e-04 - val_loss: 4.7861e-04\n",
            "Epoch 195/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1525e-04 - val_loss: 4.5888e-04\n",
            "Epoch 196/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 6.1702e-04 - val_loss: 4.6415e-04\n",
            "Epoch 197/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - loss: 6.0980e-04 - val_loss: 4.5718e-04\n",
            "Epoch 198/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 6.1521e-04 - val_loss: 4.8538e-04\n",
            "Epoch 199/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 6.1949e-04 - val_loss: 4.5415e-04\n",
            "Epoch 200/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945us/step - loss: 6.1244e-04 - val_loss: 4.5567e-04\n",
            "Epoch 201/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 6.1447e-04 - val_loss: 4.5765e-04\n",
            "Epoch 202/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - loss: 6.1177e-04 - val_loss: 4.7219e-04\n",
            "Epoch 203/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 6.0811e-04 - val_loss: 4.4687e-04\n",
            "Epoch 204/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 6.1247e-04 - val_loss: 4.7056e-04\n",
            "Epoch 205/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 6.0994e-04 - val_loss: 4.5278e-04\n",
            "Epoch 206/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 6.0670e-04 - val_loss: 4.6290e-04\n",
            "Epoch 207/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 6.0589e-04 - val_loss: 4.5267e-04\n",
            "Epoch 208/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 6.1277e-04 - val_loss: 4.6307e-04\n",
            "Epoch 209/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 6.0451e-04 - val_loss: 4.5526e-04\n",
            "Epoch 210/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 6.1278e-04 - val_loss: 4.6829e-04\n",
            "Epoch 211/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 6.0398e-04 - val_loss: 4.5346e-04\n",
            "Epoch 212/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 6.1043e-04 - val_loss: 4.8322e-04\n",
            "Epoch 213/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0018e-04 - val_loss: 4.5853e-04\n",
            "Epoch 214/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1102e-04 - val_loss: 4.6476e-04\n",
            "Epoch 215/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0605e-04 - val_loss: 4.5773e-04\n",
            "Epoch 216/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1136e-04 - val_loss: 4.8120e-04\n",
            "Epoch 217/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0079e-04 - val_loss: 4.4796e-04\n",
            "Epoch 218/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 6.0847e-04 - val_loss: 4.7041e-04\n",
            "Epoch 219/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 6.0277e-04 - val_loss: 4.4910e-04\n",
            "Epoch 220/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - loss: 6.0415e-04 - val_loss: 4.6976e-04\n",
            "Epoch 221/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0449e-04 - val_loss: 4.6773e-04\n",
            "Epoch 222/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0442e-04 - val_loss: 4.4836e-04\n",
            "Epoch 223/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 6.0529e-04 - val_loss: 4.5025e-04\n",
            "Epoch 224/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 6.1493e-04 - val_loss: 4.4681e-04\n",
            "Epoch 225/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 6.0581e-04 - val_loss: 4.4567e-04\n",
            "Epoch 226/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 5.9766e-04 - val_loss: 4.6975e-04\n",
            "Epoch 227/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 6.0900e-04 - val_loss: 4.7115e-04\n",
            "Epoch 228/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 6.0200e-04 - val_loss: 4.6362e-04\n",
            "Epoch 229/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 6.1702e-04 - val_loss: 4.7220e-04\n",
            "Epoch 230/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1974e-04 - val_loss: 4.7690e-04\n",
            "Epoch 231/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 6.0464e-04 - val_loss: 4.5096e-04\n",
            "Epoch 232/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 5.9633e-04 - val_loss: 4.6600e-04\n",
            "Epoch 233/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 5.9708e-04 - val_loss: 4.6607e-04\n",
            "Epoch 234/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 5.9857e-04 - val_loss: 4.6079e-04\n",
            "Epoch 235/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 5.9385e-04 - val_loss: 4.3643e-04\n",
            "Epoch 236/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 5.9804e-04 - val_loss: 4.7699e-04\n",
            "Epoch 237/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 5.9670e-04 - val_loss: 4.4232e-04\n",
            "Epoch 238/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 5.9365e-04 - val_loss: 4.8670e-04\n",
            "Epoch 239/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 5.9493e-04 - val_loss: 4.5589e-04\n",
            "Epoch 240/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 5.9301e-04 - val_loss: 4.6226e-04\n",
            "Epoch 241/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9454e-04 - val_loss: 4.5084e-04\n",
            "Epoch 242/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 5.9697e-04 - val_loss: 4.6352e-04\n",
            "Epoch 243/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 5.9435e-04 - val_loss: 4.7659e-04\n",
            "Epoch 244/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - loss: 5.9509e-04 - val_loss: 4.5500e-04\n",
            "Epoch 245/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - loss: 5.9171e-04 - val_loss: 4.7451e-04\n",
            "Epoch 246/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 5.9661e-04 - val_loss: 4.4918e-04\n",
            "Epoch 247/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9699e-04 - val_loss: 4.6722e-04\n",
            "Epoch 248/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - loss: 5.9748e-04 - val_loss: 4.5622e-04\n",
            "Epoch 249/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 5.8901e-04 - val_loss: 4.6488e-04\n",
            "Epoch 250/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - loss: 5.9495e-04 - val_loss: 4.7387e-04\n",
            "Epoch 251/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 5.9000e-04 - val_loss: 4.6676e-04\n",
            "Epoch 252/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 5.9296e-04 - val_loss: 4.5401e-04\n",
            "Epoch 253/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 5.8984e-04 - val_loss: 4.8825e-04\n",
            "Epoch 254/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 5.9453e-04 - val_loss: 4.5791e-04\n",
            "Epoch 255/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 5.9249e-04 - val_loss: 4.7451e-04\n",
            "Epoch 256/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - loss: 5.9751e-04 - val_loss: 4.5530e-04\n",
            "Epoch 257/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 5.9121e-04 - val_loss: 4.4512e-04\n",
            "Epoch 258/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9363e-04 - val_loss: 4.5143e-04\n",
            "Epoch 259/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 5.9410e-04 - val_loss: 4.9025e-04\n",
            "Epoch 260/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9748e-04 - val_loss: 4.6735e-04\n",
            "Epoch 261/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 5.8344e-04 - val_loss: 5.4699e-04\n",
            "Epoch 262/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 5.9439e-04 - val_loss: 4.4239e-04\n",
            "Epoch 263/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 5.9292e-04 - val_loss: 4.9216e-04\n",
            "Epoch 264/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8861e-04 - val_loss: 4.6792e-04\n",
            "Epoch 265/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 5.8689e-04 - val_loss: 4.5635e-04\n",
            "Epoch 266/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 5.8734e-04 - val_loss: 4.5258e-04\n",
            "Epoch 267/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 5.8371e-04 - val_loss: 4.6699e-04\n",
            "Epoch 268/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 5.8659e-04 - val_loss: 4.6072e-04\n",
            "Epoch 269/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 5.8564e-04 - val_loss: 4.6700e-04\n",
            "Epoch 270/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - loss: 5.8226e-04 - val_loss: 4.8613e-04\n",
            "Epoch 271/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 5.8327e-04 - val_loss: 4.6855e-04\n",
            "Epoch 272/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8460e-04 - val_loss: 4.5282e-04\n",
            "Epoch 273/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7931e-04 - val_loss: 4.4127e-04\n",
            "Epoch 274/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 5.8081e-04 - val_loss: 4.9488e-04\n",
            "Epoch 275/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 5.9144e-04 - val_loss: 4.5154e-04\n",
            "Epoch 276/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8255e-04 - val_loss: 4.6749e-04\n",
            "Epoch 277/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 5.7858e-04 - val_loss: 4.6552e-04\n",
            "Epoch 278/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 5.8109e-04 - val_loss: 4.3887e-04\n",
            "Epoch 279/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7898e-04 - val_loss: 4.8247e-04\n",
            "Epoch 280/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 5.8427e-04 - val_loss: 4.3755e-04\n",
            "Epoch 281/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8402e-04 - val_loss: 4.7951e-04\n",
            "Epoch 282/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8434e-04 - val_loss: 4.4198e-04\n",
            "Epoch 283/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 5.7741e-04 - val_loss: 4.6366e-04\n",
            "Epoch 284/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7441e-04 - val_loss: 4.3898e-04\n",
            "Epoch 285/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7545e-04 - val_loss: 4.7804e-04\n",
            "Epoch 286/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 5.8135e-04 - val_loss: 4.4779e-04\n",
            "Epoch 287/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 5.7746e-04 - val_loss: 4.8781e-04\n",
            "Epoch 288/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 5.7697e-04 - val_loss: 4.4308e-04\n",
            "Epoch 289/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - loss: 5.7593e-04 - val_loss: 4.7423e-04\n",
            "Epoch 290/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 5.8032e-04 - val_loss: 4.5075e-04\n",
            "Epoch 291/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 5.7883e-04 - val_loss: 4.5620e-04\n",
            "Epoch 292/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 5.7523e-04 - val_loss: 4.6396e-04\n",
            "Epoch 293/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 5.7656e-04 - val_loss: 4.7214e-04\n",
            "Epoch 294/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 5.7807e-04 - val_loss: 4.5874e-04\n",
            "Epoch 295/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 5.7647e-04 - val_loss: 4.5353e-04\n",
            "Epoch 296/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 5.7274e-04 - val_loss: 4.7681e-04\n",
            "Epoch 297/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - loss: 5.7579e-04 - val_loss: 4.3402e-04\n",
            "Epoch 298/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 5.7555e-04 - val_loss: 4.9651e-04\n",
            "Epoch 299/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 5.8202e-04 - val_loss: 4.5352e-04\n",
            "Epoch 300/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 5.7476e-04 - val_loss: 4.5762e-04\n",
            "Epoch 301/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 5.7281e-04 - val_loss: 4.7609e-04\n",
            "Epoch 302/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - loss: 5.7276e-04 - val_loss: 4.8099e-04\n",
            "Epoch 303/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 5.7086e-04 - val_loss: 4.6427e-04\n",
            "Epoch 304/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - loss: 5.7242e-04 - val_loss: 4.7670e-04\n",
            "Epoch 305/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 5.7730e-04 - val_loss: 4.5724e-04\n",
            "Epoch 306/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 5.7333e-04 - val_loss: 4.4811e-04\n",
            "Epoch 307/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7294e-04 - val_loss: 4.5890e-04\n",
            "Epoch 308/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 5.7482e-04 - val_loss: 4.7270e-04\n",
            "Epoch 309/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7291e-04 - val_loss: 4.7868e-04\n",
            "Epoch 310/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 5.6847e-04 - val_loss: 4.5406e-04\n",
            "Epoch 311/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 5.7175e-04 - val_loss: 5.0860e-04\n",
            "Epoch 312/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 5.7067e-04 - val_loss: 4.6731e-04\n",
            "Epoch 313/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 5.6887e-04 - val_loss: 4.5860e-04\n",
            "Epoch 314/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 5.6704e-04 - val_loss: 4.4637e-04\n",
            "Epoch 315/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 5.7253e-04 - val_loss: 4.7947e-04\n",
            "Epoch 316/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 5.7491e-04 - val_loss: 4.4440e-04\n",
            "Epoch 317/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 5.7745e-04 - val_loss: 4.9080e-04\n",
            "Epoch 318/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 5.8267e-04 - val_loss: 4.6493e-04\n",
            "Epoch 319/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 5.7475e-04 - val_loss: 4.8303e-04\n",
            "Epoch 320/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 5.6968e-04 - val_loss: 4.8832e-04\n",
            "Epoch 321/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 5.6847e-04 - val_loss: 4.6146e-04\n",
            "Epoch 322/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 5.6593e-04 - val_loss: 4.8052e-04\n",
            "Epoch 323/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - loss: 5.7276e-04 - val_loss: 4.6096e-04\n",
            "Epoch 324/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 5.6447e-04 - val_loss: 4.6605e-04\n",
            "Epoch 325/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 5.6653e-04 - val_loss: 4.7178e-04\n",
            "Epoch 326/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - loss: 5.6787e-04 - val_loss: 4.6251e-04\n",
            "Epoch 327/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 5.6768e-04 - val_loss: 4.5018e-04\n",
            "Epoch 328/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 5.7044e-04 - val_loss: 5.1956e-04\n",
            "Epoch 329/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8107e-04 - val_loss: 4.3791e-04\n",
            "Epoch 330/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - loss: 5.7861e-04 - val_loss: 4.7991e-04\n",
            "Epoch 331/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 5.8312e-04 - val_loss: 4.6600e-04\n",
            "Epoch 332/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - loss: 5.7036e-04 - val_loss: 4.4546e-04\n",
            "Epoch 333/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6963e-04 - val_loss: 5.1492e-04\n",
            "Epoch 334/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 5.7487e-04 - val_loss: 4.4713e-04\n",
            "Epoch 335/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 5.7744e-04 - val_loss: 5.1039e-04\n",
            "Epoch 336/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 5.8946e-04 - val_loss: 4.6500e-04\n",
            "Epoch 337/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 5.7589e-04 - val_loss: 4.8949e-04\n",
            "Epoch 338/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 5.6786e-04 - val_loss: 4.6318e-04\n",
            "Epoch 339/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 5.6066e-04 - val_loss: 4.7319e-04\n",
            "Epoch 340/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 5.5903e-04 - val_loss: 4.7371e-04\n",
            "Epoch 341/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 5.6026e-04 - val_loss: 4.5546e-04\n",
            "Epoch 342/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 5.6307e-04 - val_loss: 4.4005e-04\n",
            "Epoch 343/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 5.6150e-04 - val_loss: 4.6310e-04\n",
            "Epoch 344/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 5.6158e-04 - val_loss: 4.7642e-04\n",
            "Epoch 345/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 5.6105e-04 - val_loss: 4.8365e-04\n",
            "Epoch 346/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 5.6153e-04 - val_loss: 4.6787e-04\n",
            "Epoch 347/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 5.5679e-04 - val_loss: 4.8040e-04\n",
            "Epoch 348/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 5.6014e-04 - val_loss: 4.7493e-04\n",
            "Epoch 349/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 5.6221e-04 - val_loss: 4.7807e-04\n",
            "Epoch 350/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6219e-04 - val_loss: 4.4893e-04\n",
            "Epoch 351/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 5.5756e-04 - val_loss: 4.6924e-04\n",
            "Epoch 352/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 5.5954e-04 - val_loss: 4.6662e-04\n",
            "Epoch 353/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 5.6081e-04 - val_loss: 4.4968e-04\n",
            "Epoch 354/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - loss: 5.6392e-04 - val_loss: 4.5580e-04\n",
            "Epoch 355/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 5.6105e-04 - val_loss: 4.7635e-04\n",
            "Epoch 356/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909us/step - loss: 5.5811e-04 - val_loss: 4.7501e-04\n",
            "Epoch 357/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 5.5932e-04 - val_loss: 4.6739e-04\n",
            "Epoch 358/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 5.5942e-04 - val_loss: 4.8728e-04\n",
            "Epoch 359/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5784e-04 - val_loss: 4.9796e-04\n",
            "Epoch 360/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5917e-04 - val_loss: 4.7731e-04\n",
            "Epoch 361/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6031e-04 - val_loss: 5.1303e-04\n",
            "Epoch 362/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 5.6496e-04 - val_loss: 4.2912e-04\n",
            "Epoch 363/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 5.5430e-04 - val_loss: 4.7972e-04\n",
            "Epoch 364/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 5.5615e-04 - val_loss: 4.9398e-04\n",
            "Epoch 365/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 5.5527e-04 - val_loss: 4.5281e-04\n",
            "Epoch 366/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 5.5239e-04 - val_loss: 4.6715e-04\n",
            "Epoch 367/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 5.5599e-04 - val_loss: 4.7954e-04\n",
            "Epoch 368/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5564e-04 - val_loss: 4.8428e-04\n",
            "Epoch 369/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 5.5816e-04 - val_loss: 4.9678e-04\n",
            "Epoch 370/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 5.5467e-04 - val_loss: 4.7586e-04\n",
            "Epoch 371/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - loss: 5.5661e-04 - val_loss: 5.0124e-04\n",
            "Epoch 372/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 5.7263e-04 - val_loss: 4.3195e-04\n",
            "Epoch 373/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 5.7527e-04 - val_loss: 4.9852e-04\n",
            "Epoch 374/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 5.7050e-04 - val_loss: 4.2212e-04\n",
            "Epoch 375/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 5.5721e-04 - val_loss: 4.7737e-04\n",
            "Epoch 376/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 5.6089e-04 - val_loss: 4.5707e-04\n",
            "Epoch 377/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 5.5761e-04 - val_loss: 4.7853e-04\n",
            "Epoch 378/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 5.6544e-04 - val_loss: 4.8188e-04\n",
            "Epoch 379/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 5.6294e-04 - val_loss: 4.6499e-04\n",
            "Epoch 380/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 5.6029e-04 - val_loss: 5.0112e-04\n",
            "Epoch 381/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 5.6275e-04 - val_loss: 4.8119e-04\n",
            "Epoch 382/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 5.6361e-04 - val_loss: 5.4571e-04\n",
            "Epoch 383/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 5.6394e-04 - val_loss: 4.8972e-04\n",
            "Epoch 384/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 5.5954e-04 - val_loss: 4.5897e-04\n",
            "Epoch 385/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 5.5681e-04 - val_loss: 4.6390e-04\n",
            "Epoch 386/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 5.5972e-04 - val_loss: 4.7526e-04\n",
            "Epoch 387/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 5.5939e-04 - val_loss: 4.6037e-04\n",
            "Epoch 388/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 5.5537e-04 - val_loss: 4.8469e-04\n",
            "Epoch 389/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 5.6291e-04 - val_loss: 4.9104e-04\n",
            "Epoch 390/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5618e-04 - val_loss: 5.0672e-04\n",
            "Epoch 391/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - loss: 5.5020e-04 - val_loss: 4.9188e-04\n",
            "Epoch 392/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 5.4831e-04 - val_loss: 5.1108e-04\n",
            "Epoch 393/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 5.5337e-04 - val_loss: 4.6639e-04\n",
            "Epoch 394/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 5.5401e-04 - val_loss: 5.1777e-04\n",
            "Epoch 395/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 5.4846e-04 - val_loss: 5.1005e-04\n",
            "Epoch 396/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 5.5155e-04 - val_loss: 5.3273e-04\n",
            "Epoch 397/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 5.5408e-04 - val_loss: 4.9815e-04\n",
            "Epoch 398/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 5.5390e-04 - val_loss: 4.8908e-04\n",
            "Epoch 399/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 5.5193e-04 - val_loss: 4.8417e-04\n",
            "Epoch 400/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 5.5205e-04 - val_loss: 5.4342e-04\n",
            "Epoch 401/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 5.6305e-04 - val_loss: 4.7225e-04\n",
            "Epoch 402/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 5.4978e-04 - val_loss: 4.5980e-04\n",
            "Epoch 403/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 5.5749e-04 - val_loss: 5.0705e-04\n",
            "Epoch 404/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 5.5453e-04 - val_loss: 4.9675e-04\n",
            "Epoch 405/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 5.5450e-04 - val_loss: 5.7714e-04\n",
            "Epoch 406/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 5.5911e-04 - val_loss: 5.0906e-04\n",
            "Epoch 407/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 5.5473e-04 - val_loss: 5.0668e-04\n",
            "Epoch 408/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 5.4545e-04 - val_loss: 4.9840e-04\n",
            "Epoch 409/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 5.4871e-04 - val_loss: 5.2972e-04\n",
            "Epoch 410/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 5.5068e-04 - val_loss: 4.9560e-04\n",
            "Epoch 411/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 5.5517e-04 - val_loss: 4.9861e-04\n",
            "Epoch 412/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4951e-04 - val_loss: 5.0028e-04\n",
            "Epoch 413/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 5.5037e-04 - val_loss: 4.8588e-04\n",
            "Epoch 414/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - loss: 5.4633e-04 - val_loss: 4.6594e-04\n",
            "Epoch 415/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 5.4866e-04 - val_loss: 5.0383e-04\n",
            "Epoch 416/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 5.4970e-04 - val_loss: 4.5898e-04\n",
            "Epoch 417/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - loss: 5.4676e-04 - val_loss: 4.3683e-04\n",
            "Epoch 418/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 5.5443e-04 - val_loss: 4.4747e-04\n",
            "Epoch 419/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 5.5113e-04 - val_loss: 5.0821e-04\n",
            "Epoch 420/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 5.4709e-04 - val_loss: 4.7390e-04\n",
            "Epoch 421/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - loss: 5.4450e-04 - val_loss: 4.4730e-04\n",
            "Epoch 422/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 5.4694e-04 - val_loss: 4.7910e-04\n",
            "Epoch 423/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 5.5680e-04 - val_loss: 5.2678e-04\n",
            "Epoch 424/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 5.5085e-04 - val_loss: 4.6171e-04\n",
            "Epoch 425/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 5.4762e-04 - val_loss: 4.7296e-04\n",
            "Epoch 426/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 5.4609e-04 - val_loss: 4.4094e-04\n",
            "Epoch 427/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915us/step - loss: 5.4744e-04 - val_loss: 4.6743e-04\n",
            "Epoch 428/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 5.4786e-04 - val_loss: 4.3687e-04\n",
            "Epoch 429/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 5.4677e-04 - val_loss: 4.7040e-04\n",
            "Epoch 430/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 5.4307e-04 - val_loss: 4.5755e-04\n",
            "Epoch 431/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 5.4866e-04 - val_loss: 5.3904e-04\n",
            "Epoch 432/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 5.5558e-04 - val_loss: 4.8056e-04\n",
            "Epoch 433/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4450e-04 - val_loss: 5.2843e-04\n",
            "Epoch 434/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 5.4388e-04 - val_loss: 4.5033e-04\n",
            "Epoch 435/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 5.4721e-04 - val_loss: 4.7751e-04\n",
            "Epoch 436/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 5.5336e-04 - val_loss: 4.3528e-04\n",
            "Epoch 437/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 5.6224e-04 - val_loss: 4.4520e-04\n",
            "Epoch 438/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 5.5803e-04 - val_loss: 4.7678e-04\n",
            "Epoch 439/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 5.5905e-04 - val_loss: 4.7959e-04\n",
            "Epoch 440/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 5.5688e-04 - val_loss: 4.9156e-04\n",
            "Epoch 441/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 5.6021e-04 - val_loss: 4.4139e-04\n",
            "Epoch 442/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 5.6070e-04 - val_loss: 4.6444e-04\n",
            "Epoch 443/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 5.5571e-04 - val_loss: 4.3965e-04\n",
            "Epoch 444/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 5.6559e-04 - val_loss: 4.6112e-04\n",
            "Epoch 445/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 5.5727e-04 - val_loss: 4.4078e-04\n",
            "Epoch 446/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 5.5931e-04 - val_loss: 4.4220e-04\n",
            "Epoch 447/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - loss: 5.5943e-04 - val_loss: 4.5538e-04\n",
            "Epoch 448/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 5.4746e-04 - val_loss: 4.6380e-04\n",
            "Epoch 449/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 5.5675e-04 - val_loss: 4.7763e-04\n",
            "Epoch 450/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 5.5612e-04 - val_loss: 4.9900e-04\n",
            "Epoch 451/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - loss: 5.5455e-04 - val_loss: 5.1068e-04\n",
            "Epoch 452/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 5.5459e-04 - val_loss: 4.4547e-04\n",
            "Epoch 453/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 5.5557e-04 - val_loss: 4.3113e-04\n",
            "Epoch 454/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 5.6142e-04 - val_loss: 4.5149e-04\n",
            "Epoch 455/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 5.6457e-04 - val_loss: 4.9531e-04\n",
            "Epoch 456/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 5.5660e-04 - val_loss: 4.8245e-04\n",
            "Epoch 457/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 5.8296e-04 - val_loss: 5.1715e-04\n",
            "Epoch 458/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 5.5899e-04 - val_loss: 4.7394e-04\n",
            "Epoch 459/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 5.7674e-04 - val_loss: 4.4974e-04\n",
            "Epoch 460/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5889e-04 - val_loss: 4.4232e-04\n",
            "Epoch 461/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 5.6037e-04 - val_loss: 4.3543e-04\n",
            "Epoch 462/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 5.5964e-04 - val_loss: 4.4055e-04\n",
            "Epoch 463/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 5.6137e-04 - val_loss: 5.0893e-04\n",
            "Epoch 464/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 5.5754e-04 - val_loss: 4.5831e-04\n",
            "Epoch 465/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 5.6244e-04 - val_loss: 4.3311e-04\n",
            "Epoch 466/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 5.6920e-04 - val_loss: 4.2888e-04\n",
            "Epoch 467/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 5.5912e-04 - val_loss: 4.8730e-04\n",
            "Epoch 468/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 5.6797e-04 - val_loss: 4.3174e-04\n",
            "Epoch 469/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 5.6589e-04 - val_loss: 4.6272e-04\n",
            "Epoch 470/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 5.6633e-04 - val_loss: 4.9842e-04\n",
            "Epoch 471/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 5.8027e-04 - val_loss: 4.4801e-04\n",
            "Epoch 472/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step - loss: 5.5912e-04 - val_loss: 4.3210e-04\n",
            "Epoch 473/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 5.8304e-04 - val_loss: 4.2785e-04\n",
            "Epoch 474/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - loss: 5.7048e-04 - val_loss: 4.8178e-04\n",
            "Epoch 475/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 5.7421e-04 - val_loss: 4.4537e-04\n",
            "Epoch 476/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 5.6231e-04 - val_loss: 4.3145e-04\n",
            "Epoch 477/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 5.9105e-04 - val_loss: 4.5178e-04\n",
            "Epoch 478/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - loss: 5.8670e-04 - val_loss: 4.3524e-04\n",
            "Epoch 479/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 5.9636e-04 - val_loss: 4.4916e-04\n",
            "Epoch 480/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 6.2996e-04 - val_loss: 4.9196e-04\n",
            "Epoch 481/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 6.1171e-04 - val_loss: 4.2983e-04\n",
            "Epoch 482/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.3726e-04 - val_loss: 4.9156e-04\n",
            "Epoch 483/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 6.5693e-04 - val_loss: 5.4712e-04\n",
            "Epoch 484/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7500e-04 - val_loss: 5.7432e-04\n",
            "Epoch 485/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 6.9590e-04 - val_loss: 6.3515e-04\n",
            "Epoch 486/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 6.7607e-04 - val_loss: 6.0797e-04\n",
            "Epoch 487/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 6.4878e-04 - val_loss: 6.0029e-04\n",
            "Epoch 488/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 6.2231e-04 - val_loss: 5.6061e-04\n",
            "Epoch 489/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 6.2014e-04 - val_loss: 5.0779e-04\n",
            "Epoch 490/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 6.2066e-04 - val_loss: 5.0505e-04\n",
            "Epoch 491/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 6.1203e-04 - val_loss: 5.1413e-04\n",
            "Epoch 492/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 6.2159e-04 - val_loss: 5.7029e-04\n",
            "Epoch 493/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 6.1595e-04 - val_loss: 5.5562e-04\n",
            "Epoch 494/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 6.1073e-04 - val_loss: 5.4319e-04\n",
            "Epoch 495/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 5.9762e-04 - val_loss: 5.6904e-04\n",
            "Epoch 496/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 6.1161e-04 - val_loss: 5.6690e-04\n",
            "Epoch 497/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 5.9676e-04 - val_loss: 5.5822e-04\n",
            "Epoch 498/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 5.8655e-04 - val_loss: 5.3656e-04\n",
            "Epoch 499/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8825e-04 - val_loss: 5.5727e-04\n",
            "Epoch 500/500\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 5.9018e-04 - val_loss: 5.4724e-04\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation=\"relu\", input_shape=(window_size,)))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "model.add(Dense(8, activation=\"relu\"))\n",
        "model.add(Dense(4, activation=\"relu\"))\n",
        "model.add(Dense(1, activation = \"linear\"))  # salida escalar\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "history = model.fit(X_train, y_train, epochs=500, batch_size=36, validation_split=0.1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "               Real   Predicho\n",
            "Fecha                         \n",
            "2025-09-12  18.4757  18.450499\n",
            "2025-09-15  18.3635  18.385088\n",
            "2025-09-17  18.3257  18.281385\n",
            "2025-09-18  18.3610  18.234518\n",
            "2025-09-19  18.3892  18.293472\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1,1))\n",
        "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1,1))\n",
        "\n",
        "# DataFrame comparativo\n",
        "df_pred = pd.DataFrame({\n",
        "    \"Real\": y_test_rescaled.flatten(),\n",
        "    \"Predicho\": y_pred_rescaled.flatten()\n",
        "}, index=fechas_test)\n",
        "\n",
        "print(df_pred.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337us/step\n"
          ]
        }
      ],
      "source": [
        "y_pred_all = model.predict(X)  # predicciones en todo el set (train + test)\n",
        "y_pred_all_rescaled = scaler.inverse_transform(y_pred_all.reshape(-1,1))\n",
        "y_all_rescaled = scaler.inverse_transform(y.reshape(-1,1))\n",
        "\n",
        "fechas_all = df.index[window_size:]  # fechas correspondientes\n",
        "\n",
        "df_pred_all = pd.DataFrame({\n",
        "    \"Real\": y_all_rescaled.flatten(),\n",
        "    \"Predicho\": y_pred_all_rescaled.flatten()\n",
        "}, index=fechas_all)\n",
        "\n",
        "\n",
        "\n",
        "# Filtrar desde 2015\n",
        "fecha_inicio = \"2015-01-01\"\n",
        "df_filtrado = df.loc[fecha_inicio:]\n",
        "df_pred_all_filtrado = df_pred_all.loc[fecha_inicio:]\n",
        "df_pred_filtrado = df_pred.loc[fecha_inicio:]\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Serie completa real\n",
        "fig.add_trace(go.Scatter(x=df_filtrado.index, y=df_filtrado[\"TipoCambio\"],\n",
        "                         mode=\"lines\",\n",
        "                         name=\"Serie completa (Real)\",\n",
        "                         line=dict(color=\"lightgray\")))\n",
        "\n",
        "# Predicciones de todo el modelo (train + test)\n",
        "fig.add_trace(go.Scatter(x=df_pred_all_filtrado.index, y=df_pred_all_filtrado[\"Predicho\"],\n",
        "                         mode=\"lines\",\n",
        "                         name=\"Predicho (train + test)\",\n",
        "                         line=dict(color=\"orange\")))\n",
        "\n",
        "# Últimos 15 días - reales\n",
        "fig.add_trace(go.Scatter(x=df_pred_filtrado.index, y=df_pred_filtrado[\"Real\"],\n",
        "                         mode=\"lines+markers\",\n",
        "                         name=\"Real (últimos 15 días)\",\n",
        "                         line=dict(color=\"blue\")))\n",
        "\n",
        "# Últimos 15 días - predichos\n",
        "fig.add_trace(go.Scatter(x=df_pred_filtrado.index, y=df_pred_filtrado[\"Predicho\"],\n",
        "                         mode=\"lines+markers\",\n",
        "                         name=\"Predicho (últimos 15 días)\",\n",
        "                         line=dict(color=\"red\", dash=\"dot\")))\n",
        "\n",
        "fig.update_layout(title=\"Predicción de la TIIE con FNN\",\n",
        "                  xaxis_title=\"Fecha\",\n",
        "                  yaxis_title=\"TIIE 28 días (%)\",\n",
        "                  legend=dict(x=0.01, y=0.99, bordercolor=\"black\", borderwidth=1))\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
